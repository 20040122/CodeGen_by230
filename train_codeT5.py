# import torch
# from transformers import T5ForConditionalGeneration, RobertaTokenizer, get_linear_schedule_with_warmup
# from peft import get_peft_model, LoraConfig
# from datasets import load_dataset
#
# # è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨ CUDAï¼‰
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print("å½“å‰è®¾å¤‡ï¼š", device)
# # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆCodeT5-smallï¼‰å’Œ tokenizer
# model_name = "D:/PythonCode/CodeBERT/model/CodeT5-small"
# model = T5ForConditionalGeneration.from_pretrained(model_name)
# # å®šä¹‰ LoRA é…ç½®ï¼Œå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ
# peft_config = LoraConfig(
#     task_type="SEQ_2_SEQ_LM",  # åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
#     r=8,
#     lora_alpha=32,
#     lora_dropout=0.1,
# )
# model = get_peft_model(model, peft_config)
# model.to(device)
#
# tokenizer = RobertaTokenizer.from_pretrained(model_name)
#
# # åŠ è½½æ•°æ®é›†ï¼ˆè¿™é‡ŒåŠ è½½çš„æ˜¯ Python ç‰ˆæœ¬ï¼‰
# ds = load_dataset("google/code_x_glue_ct_code_to_text", "python")
# train_dataset = ds["train"].select(range(1000))
# # å¦‚æœæ•°æ®é›†å¤ªå¤§ï¼Œå¯ä»¥ä½¿ç”¨ .select() é™åˆ¶æ•°æ®é‡ç”¨äºè°ƒè¯•ï¼Œä¾‹å¦‚ï¼š
# # train_dataset = ds["train"].select(range(1000))
#
# # è®¾ç½®è®­ç»ƒå‚æ•°
# num_epochs = 3  # æ ¹æ®éœ€è¦å¯ä»¥å¢å¤§è®­ç»ƒè½®æ¬¡ï¼ˆä¾‹å¦‚ 10ã€20 æˆ–æ›´å¤šï¼‰
# learning_rate = 1e-4
# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
# total_steps = num_epochs * len(train_dataset)
# scheduler = get_linear_schedule_with_warmup(
#     optimizer,
#     num_warmup_steps=int(0.1 * total_steps),
#     num_training_steps=total_steps
# )
#
# # è®­ç»ƒå¾ªç¯
# print("å¼€å§‹è®­ç»ƒâ€¦â€¦")
# for epoch in range(num_epochs):
#     total_loss = 0.0
#     model.train()
#     for i, sample in enumerate(train_dataset):
#         # ä½¿ç”¨ dataset çš„ code å’Œ docstring å­—æ®µ
#         # è¿™é‡Œå¯ä»¥ä¸º code æ·»åŠ å‰ç¼€ "summarize: " å¸®åŠ©æ¨¡å‹ç†è§£ä»»åŠ¡ï¼ˆè§†é¢„è®­ç»ƒä»»åŠ¡è€Œå®šï¼‰
#         code = "summarize: " + sample["code"]
#         docstring = sample["docstring"]
#
#         # ç¼–ç è¾“å…¥å’Œæ ‡ç­¾ï¼ˆç¡®ä¿è¿›è¡Œæˆªæ–­ï¼Œé˜²æ­¢è¿‡é•¿ï¼‰
#         inputs = tokenizer(code, return_tensors="pt", truncation=True, max_length=512)
#         labels = tokenizer(docstring, return_tensors="pt", truncation=True, max_length=128)
#         input_ids = inputs.input_ids.to(device)
#         labels_ids = labels.input_ids.to(device)
#
#         optimizer.zero_grad()
#         outputs = model(input_ids=input_ids, labels=labels_ids)
#         loss = outputs.loss
#         loss.backward()
#         optimizer.step()
#         scheduler.step()
#
#         total_loss += loss.item()
#
#         # æ¯ 100 ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡å½“å‰æŸå¤±
#         if (i + 1) % 100 == 0:
#             print(f"Epoch {epoch+1}/{num_epochs} - Step {i+1}/{len(train_dataset)} - Loss: {loss.item():.4f}")
#
#     avg_loss = total_loss / len(train_dataset)
#     print(f"Epoch {epoch+1}/{num_epochs} å®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.4f}")
# # è®­ç»ƒå®Œæˆåï¼Œå°† LoRA é€‚é…å™¨åˆå¹¶åˆ° CodeT5 æ¨¡å‹
# model = model.merge_and_unload()
#
# # ä¿å­˜å®Œæ•´æ¨¡å‹
# model_save_path = "D:/PythonCode/CodeBERT/model/finetuned_codet5_small"
# model.save_pretrained(model_save_path)
# tokenizer.save_pretrained(model_save_path)
#
# print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{model_save_path}")
import torch
from transformers import T5ForConditionalGeneration, RobertaTokenizer, get_linear_schedule_with_warmup
from peft import get_peft_model, LoraConfig
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import default_data_collator

# è®¾ç½®è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨ CUDAï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("å½“å‰è®¾å¤‡ï¼š", device)

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆCodeT5-smallï¼‰å’Œ tokenizer
model_name = "D:/PythonCode/CodeBERT/model/CodeT5-small"
model = T5ForConditionalGeneration.from_pretrained(model_name)

# å®šä¹‰ LoRA é…ç½®
peft_config = LoraConfig(
    task_type="SEQ_2_SEQ_LM",
    r=8,  # LoRA ä½ç§©ç»´åº¦
    lora_alpha=32,
    lora_dropout=0.1,
)
model = get_peft_model(model, peft_config)
model.to(device)

tokenizer = RobertaTokenizer.from_pretrained(model_name)

# åŠ è½½æ•°æ®é›†
ds = load_dataset("google/code_x_glue_ct_code_to_text", "python")
train_dataset = ds["train"]  # 25 ä¸‡æ¡æ•°æ®


# **ä¿®æ­£æ‰¹é‡é¢„å¤„ç†å‡½æ•°**
def preprocess_function(sample):
    """å¯¹ batch æ•°æ®è¿›è¡Œé¢„å¤„ç†"""
    return {
        "input_ids": [tokenizer("summarize: " + c, truncation=True, max_length=256, padding="max_length")["input_ids"]
                      for c in sample["code"]],
        "labels": [tokenizer(d, truncation=True, max_length=64, padding="max_length")["input_ids"] for d in
                   sample["docstring"]]
    }


# **ä½¿ç”¨ map() è¿›è¡Œæ‰¹é‡é¢„å¤„ç†**
train_dataset = train_dataset.map(preprocess_function, batched=True,
                                  remove_columns=["id", "repo", "path", "func_name", "original_string", "language",
                                                  "code", "code_tokens", "docstring", "docstring_tokens", "sha", "url"])

# **åˆ›å»º DataLoader**
batch_size = 4
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=default_data_collator)

# **è®­ç»ƒå‚æ•°**
num_epochs = 10
learning_rate = 3e-5
gradient_accumulation_steps = 8

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
total_steps = len(train_dataloader) * num_epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps),
                                            num_training_steps=total_steps)

# **è®­ç»ƒå¾ªç¯**
print("ğŸš€ å¼€å§‹è®­ç»ƒâ€¦â€¦")

scaler = torch.cuda.amp.GradScaler()  # FP16 è®­ç»ƒ

for epoch in range(num_epochs):
    total_loss = 0.0
    model.train()

    for step, batch in enumerate(train_dataloader):
        batch = {k: v.to(device) for k, v in batch.items()}

        with torch.cuda.amp.autocast():  # **å¯ç”¨æ··åˆç²¾åº¦**
            outputs = model(**batch)
            loss = outputs.loss / gradient_accumulation_steps  # å¤„ç†æ¢¯åº¦ç´¯ç§¯

        scaler.scale(loss).backward()

        if (step + 1) % gradient_accumulation_steps == 0:  # **ç´¯ç§¯å¤šä¸ª batch åæ›´æ–°**
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            scheduler.step()

        total_loss += loss.item()

        if (step + 1) % 100 == 0:
            print(f"Epoch {epoch + 1}/{num_epochs} - Step {step + 1}/{len(train_dataloader)} - Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(train_dataloader)
    print(f"âœ… Epoch {epoch + 1}/{num_epochs} å®Œæˆ - å¹³å‡æŸå¤±: {avg_loss:.4f}")

# **åˆå¹¶ LoRA é€‚é…å™¨å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹**
model = model.merge_and_unload()
model.save_pretrained("D:/PythonCode/CodeBERT/model/finetuned_codet5_small")
tokenizer.save_pretrained("D:/PythonCode/CodeBERT/model/finetuned_codet5_small")

print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜ï¼")
